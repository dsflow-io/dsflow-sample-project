# <markdowncell>

# *Note: this notebook was generated by dsflow*
#
# # create table {{ dataset_name }}
#
# ## \[1- init and config\] load tables and set output path

# <codecell>

import os
import json
import datetime as dt
from pandasql import PandaSQL
from custom_libraries.helpers import show_tables, load_table

# <codecell>

# default task parameters
default_task_specs = """
    {"sink_path": "/data/tables/{{ dataset_name }}/ds={{ ds }}",
     "ds": "{{ ds }}"}
    """

task_specs = json.loads(os.environ.get('TASK_SPECS', default_task_specs))

# <markdowncell>


# ## \[2- process data\] process data based on SQL

# You can load contents of datastore/tables/ with
#
# `my_table_name = load_table(type="parquet", table_name="my_table_name")`
#
# You can load contents of datastore/raw with
#
# `my_table_name = load_table(type="json", table_name="my_table_name")`
#
# or `my_table_name = load_table(type="csv", table_name="my_table_name")`
# 
# using `PandaSQL()` you can then query `my_table_name` using a SQL-like syntax

# <codecell>

# show tables (parquet files located in datastore/tables/*)
show_tables()

# <codecell>

# load table
my_table_name = load_table(type="parquet", table_name="my_table_name")

# <codecell>

my_table_name.head()

# <codecell>

psql = PandaSQL()

{{ dataset_name }}_df = psql("""

  SELECT *
  FROM my_table_name
  LIMIT 100

""")

{{ dataset_name }}_df

# <codecell>

{{ dataset_name }}_df.describe()


# <markdowncell>

# ## \[3- write on disk\] write the output as parquet

# <codecell>

base_dir = "/data/tables/{{ dataset_name }}/ds={{ ds }}"

if not os.path.exists(base_dir):
    os.makedirs(base_dir)

{{ dataset_name }}_df.to_parquet(task_specs["sink_path"] + "/data.parquet")
