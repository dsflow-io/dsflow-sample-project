# <markdowncell>

# *Note: this notebook was generated by dsflow*
#
# # create table {{ dataset_name }}
#
# ## \[1- init and config\] load tables and set output path

# <codecell>

import os
import json
import datetime as dt
import glob
import pandas as pd
from pandasql import PandaSQL
from custom_libraries.helpers import show_tables, load_table

# <codecell>

# default task parameters

default_task_specs = """
    {"source_path": "/data/raw/{{ dataset_name }}/ds={{ ds }}",
     "sink_path": "/data/tables/{{ dataset_name }}/ds={{ ds }}",
     "ds": "{{ ds }}"}
    """

task_specs = json.loads(os.environ.get('TASK_SPECS', default_task_specs))

# <markdowncell>


# ## \[2- process data\] import and process data

# <codecell>

# load table
all_files = glob.glob(os.path.join(task_specs["source_path"], "*"))
df_from_each_file = (pd.read_csv(f) for f in all_files)

{{ dataset_name }}_df = pd.concat(df_from_each_file, ignore_index=True)

# <codecell>

{{ dataset_name }}_df.head()

# <codecell>

# add data checks / perform SQL queries
psql = PandaSQL()

psql("""

  SELECT *
  FROM {{ dataset_name }}_df
  LIMIT 10

""")


# <codecell>

{{ dataset_name }}_df.describe()


# <markdowncell>

# ## \[3- write on disk\] write the output as parquet

# <codecell>

base_dir = "/data/tables/{{ dataset_name }}/ds={{ ds }}"

if not os.path.exists(base_dir):
    os.makedirs(base_dir)

{{ dataset_name }}_df.to_parquet(task_specs["sink_path"] + "/data.parquet")
